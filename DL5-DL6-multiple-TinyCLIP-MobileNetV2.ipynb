{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8652578",
   "metadata": {},
   "source": [
    "# VLM/MLLM D5/D6 Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a28f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter '1. single' for single dataset or '2. multi' for multiple dataset: 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (Activation, Attention, Bidirectional, Concatenate, Conv1D,\n",
    "                           Dense, Dropout, Embedding, Flatten, GlobalMaxPooling1D,\n",
    "                           Input, Layer, LSTM, MaxPooling1D, Multiply, Permute,\n",
    "                           RepeatVector, Reshape, SpatialDropout1D, TimeDistributed)\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import sys\n",
    "sys.path.append('./cs16')\n",
    "import cs16.prep as prep16\n",
    "import cs16.plot as plot16\n",
    "import cs16.build as build16\n",
    "imagesize = 64\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data_type = input(\"Enter '1. single' for single dataset or '2. multi' for multiple dataset: \")\n",
    "data_type = '2'\n",
    "if data_type == '1':\n",
    "    file_path = 'single.txt'\n",
    "    folder_path = './data/MVSA/single/'\n",
    "elif data_type == '2':\n",
    "    file_path = 'multi.txt'\n",
    "    folder_path = './data/MVSA/multiple/'\n",
    "else:\n",
    "    print(\"Invalid input. Please enter either 'single' or 'multi'.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49123e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Download stopwords and punkt tokenizer if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def nlp_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs using regex\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem words using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the preprocess_text function to the 'tweet' column of the dataframe\n",
    "df['tweet'] = df['tweet'].apply(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5e4873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgsize=96\n",
    "X_text, y_text = prep16.preprocess_text(df)\n",
    "X_train_text, X_val_text, X_test_text, \\\n",
    "y_train_text, y_val_text, y_test_text = prep16.split_data(X_text, y_text, random_state=42)\n",
    "\n",
    "X_polar, y_polar = prep16.preprocess_text(df,label = 'polarity')\n",
    "X_train_polar, X_val_polar, X_test_polar, \\\n",
    "y_train_polar, y_val_polar, y_test_polar = prep16.split_data(X_polar, y_polar, random_state=42)\n",
    "\n",
    "image_data_s, image_label_s = prep16.preprocess_images(df, folder_path, imagesize =imgsize)\n",
    "y_s = to_categorical(image_label_s, num_classes=3)\n",
    "\n",
    "X_train_image, X_val_image, X_test_image, \\\n",
    "y_train_image, y_val_image, y_test_image= prep16.split_data(image_data_s, y_s, random_state=42)\n",
    "\n",
    "y_train = to_categorical(y_train_polar, num_classes=3)\n",
    "y_val =to_categorical(y_val_polar, num_classes=3)\n",
    "y_test =to_categorical(y_test_polar, num_classes=3)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0580922c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4869, 96, 96, 3), TensorShape([3895, 96, 96, 3]), (4869, 3))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data_s.shape, X_train_image.shape, y_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddada0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from transformers import AutoTokenizer, TFCLIPModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from transformers import CLIPProcessor, CLIPModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "07d17e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1698cfb6",
   "metadata": {},
   "source": [
    "# 1. EARLY FUSION TinyCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffb4e571d6745ec81783507e5c3e615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/462M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer Start\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TinyCLIP/CLIP TEXT\n",
    "# ------------------------------------------------------------\n",
    "text_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
    "#\"openai/clip-vit-base-patch16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "#text_encoder = TFCLIPModel.from_pretrained(text_model_name)\n",
    "text_encoder = TFCLIPModel.from_pretrained(text_model_name, from_pt=True)\n",
    "text_encoder.trainable = False\n",
    "\n",
    "def get_text_features(texts, processor, model, maxlen=77):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=maxlen,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    features = model.get_text_features(\n",
    "        input_ids=encodings[\"input_ids\"]\n",
    "    )\n",
    "    return features.numpy()\n",
    "\n",
    "# 文本数据准备\n",
    "X_train_text = [str(t) for t in X_train_text]\n",
    "X_val_text = [str(t) for t in X_val_text]\n",
    "X_test_text = [str(t) for t in X_test_text]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MobileNetV2 IMAGE\n",
    "# ------------------------------------------------------------\n",
    "# Image\n",
    "image_input = Input(shape=(imgsize, imgsize, 3), name='image_input')\n",
    "mobilenet = tf.keras.applications.MobileNetV2(include_top=False, \n",
    "                                            weights='imagenet', \n",
    "                                            input_shape=(imgsize, imgsize, 3))\n",
    "#mobilenet.trainable = False\n",
    "image_features = mobilenet(image_input)\n",
    "\n",
    "mobilenet.trainable = True\n",
    "for layer in mobilenet.layers[:-20]:  # Only fine-tune Last 20 \n",
    "    layer.trainable = False\n",
    "\n",
    "# TEXT\n",
    "text_input = Input(shape=(77,), name='text_input')  # CLIP->512 MaxLen\n",
    "text_embedding = Dense(64, activation='relu')(text_input)  # Text Reduce Dim\n",
    "text_embedding = Reshape((1, 1, 64))(text_embedding)  # \n",
    "\n",
    "# Expand Text -> Image\n",
    "h, w = image_features.shape[1], image_features.shape[2]\n",
    "text_features_expanded = tf.keras.layers.UpSampling2D(size=(h, w))(text_embedding)\n",
    "\n",
    "# Early Fusion: Feature Level \n",
    "fusion = Concatenate(axis=-1)([image_features, text_features_expanded])\n",
    "\n",
    "# Reduce Dim\n",
    "x = GlobalAveragePooling2D()(fusion)\n",
    "x = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "opti = tf.keras.optimizers.AdamW(learning_rate=1e-5, clipnorm=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Extract\n",
    "X_train_text_feat = get_text_features(X_train_text, tokenizer, text_encoder)\n",
    "X_val_text_feat = get_text_features(X_val_text, tokenizer, text_encoder)\n",
    "X_test_text_feat = get_text_features(X_test_text, tokenizer, text_encoder)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training\n",
    "# ------------------------------------------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_text_feat, X_train_image], y_train,\n",
    "    validation_data=([X_val_text_feat, X_val_image], y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Report\n",
    "# ------------------------------------------------------------\n",
    "predictions_prob = model.predict([X_test_text_feat, X_test_image])\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer End\n",
    "end_time = time.time()\n",
    "print(f\"TinyCLIP+MobileNetV2- EARLY Fusion：{end_time - start_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d5795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c540f2b",
   "metadata": {},
   "source": [
    "# 2. LATE FUSION TinyCLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9d0ef",
   "metadata": {},
   "source": [
    "### out of memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9547f7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "All PyTorch model weights were used when initializing TFCLIPModel.\n",
      "\n",
      "All the weights of TFCLIPModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'mlp' (type TFCLIPMLP).\n\n{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[15568,77,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RealDiv]\n\nCall arguments received by layer 'mlp' (type TFCLIPMLP):\n  • hidden_states=tf.Tensor(shape=(15568, 77, 512), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10000/1651317661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#Extract FIRST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mX_train_text_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mX_val_text_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mX_test_text_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10000/1651317661.py\u001b[0m in \u001b[0;36mget_text_features\u001b[1;34m(texts, processor, model, maxlen)\u001b[0m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# TinyCLIP/CLIP TF 版用 get_text_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     features = model.get_text_features(\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\clip\\modeling_tf_clip.py\u001b[0m in \u001b[0;36mget_text_features\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   1183\u001b[0m         ```\"\"\"\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m         text_features = self.clip.get_text_features(\n\u001b[0m\u001b[0;32m   1186\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mrun_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Keras enforces the first layer argument to be passed, and checks it through `inspect.getfullargspec()`. This\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\clip\\modeling_tf_clip.py\u001b[0m in \u001b[0;36mget_text_features\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         text_outputs = self.text_model(\n\u001b[0m\u001b[0;32m    772\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\clip\\modeling_tf_clip.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_expand_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    521\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\clip\\modeling_tf_clip.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    461\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m             layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    464\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\clip\\modeling_tf_clip.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions, training)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\clip\\modeling_tf_clip.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'mlp' (type TFCLIPMLP).\n\n{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[15568,77,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:RealDiv]\n\nCall arguments received by layer 'mlp' (type TFCLIPMLP):\n  • hidden_states=tf.Tensor(shape=(15568, 77, 512), dtype=float32)"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer Start\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TinyCLIP/CLIP TEXT\n",
    "# ------------------------------------------------------------\n",
    "#text_model_name = \"openai/clip-vit-base-patch32\"  # 若有 TinyCLIP 的 TF 版本可替換\n",
    "text_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(text_model_name)\n",
    "text_encoder = CLIPModel.from_pretrained(text_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = TFCLIPModel.from_pretrained(text_model_name)\n",
    "# 注意：TFCLIPModel only input_ids, (not necessary) attention_mask\n",
    "text_encoder.trainable = False\n",
    "def get_text_features(texts, processor, model, maxlen=77):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=maxlen,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    # TinyCLIP/CLIP TF Verion, get_text_features\n",
    "    features = model.get_text_features(\n",
    "        input_ids=encodings[\"input_ids\"]\n",
    "    )\n",
    "    return features.numpy()\n",
    "\n",
    "# to Str\n",
    "X_train_text = [str(t) for t in X_train_text]\n",
    "X_val_text = [str(t) for t in X_val_text]\n",
    "X_test_text = [str(t) for t in X_test_text]\n",
    "\n",
    "#Extract FIRST \n",
    "X_train_text_feat = get_text_features(X_train_text, tokenizer, text_encoder)\n",
    "X_val_text_feat = get_text_features(X_val_text, tokenizer, text_encoder)\n",
    "X_test_text_feat = get_text_features(X_test_text, tokenizer, text_encoder)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MobileNetV2 IMAGE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "mobilenet = tf.keras.applications.MobileNetV2(include_top=False, \n",
    "                                              weights='imagenet', \n",
    "                                              input_shape=(imgsize, imgsize, 3))\n",
    "mobilenet.trainable = False\n",
    "image_input = Input(shape=(imgsize, imgsize, 3), name='image_input')\n",
    "#Extract\n",
    "image_features = mobilenet(image_input)\n",
    "image_features = GlobalAveragePooling2D()(image_features)\n",
    "image_features = Dense(64, activation='relu')(image_features)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fusion\n",
    "# ------------------------------------------------------------\n",
    "text_input = Input(shape=(X_train_text_feat.shape[1],), name='text_feat_input')\n",
    "fusion = Concatenate()([text_input, image_features])\n",
    "fusion = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(fusion)\n",
    "fusion = Dropout(0.5)(fusion)   \n",
    "output = Dense(3, activation='softmax')(fusion)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "opti = tf.keras.optimizers.AdamW(learning_rate=1e-4, clipnorm=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Traning\n",
    "# ------------------------------------------------------------\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_text_feat, X_train_image], y_train,\n",
    "    validation_data=([X_val_text_feat, X_val_image], y_val),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Report\n",
    "# ------------------------------------------------------------\n",
    "predictions_prob = model.predict([X_test_text_feat, X_test_image])\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer End\n",
    "end_time = time.time()\n",
    "print(f\"TinyCLIP+MobileNetV2-LATE Fusion：{end_time - start_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b4432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
