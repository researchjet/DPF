{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1af926",
   "metadata": {},
   "source": [
    "# D1-D4 Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a28f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (Activation, Attention, Bidirectional, Concatenate, Conv1D,\n",
    "                           Dense, Dropout, Embedding, Flatten, GlobalMaxPooling1D,\n",
    "                           Input, Layer, LSTM, MaxPooling1D, Multiply, Permute,\n",
    "                           RepeatVector, Reshape, SpatialDropout1D, TimeDistributed)\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import sys\n",
    "sys.path.append('./cs16')\n",
    "import cs16.prep as prep16\n",
    "import cs16.plot as plot16\n",
    "import cs16.build as build16\n",
    "imagesize = 64\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data_type = input(\"Enter '1. single' for single dataset or '2. multi' for multiple dataset: \")\n",
    "data_type = '1'\n",
    "if data_type == '1':\n",
    "    file_path = 'single.txt'\n",
    "    folder_path = './data/MVSA/single/'\n",
    "elif data_type == '2':\n",
    "    file_path = 'multi.txt'\n",
    "    folder_path = './data/MVSA/multiple/'\n",
    "else:\n",
    "    print(\"Invalid input. Please enter either 'single' or 'multi'.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49123e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Download stopwords and punkt tokenizer if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def nlp_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs using regex\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem words using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the preprocess_text function to the 'tweet' column of the dataframe\n",
    "df['tweet'] = df['tweet'].apply(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5e4873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'single.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text, y_text = prep16.preprocess_text(df)\n",
    "X_train_text, X_val_text, X_test_text, \\\n",
    "y_train_text, y_val_text, y_test_text = prep16.split_data(X_text, y_text, random_state=42)\n",
    "\n",
    "X_polar, y_polar = prep16.preprocess_text(df,label = 'polarity')\n",
    "X_train_polar, X_val_polar, X_test_polar, \\\n",
    "y_train_polar, y_val_polar, y_test_polar = prep16.split_data(X_polar, y_polar, random_state=42)\n",
    "\n",
    "image_data_s, image_label_s = prep16.preprocess_images(df, folder_path, imagesize)\n",
    "y_s = to_categorical(image_label_s, num_classes=3)\n",
    "\n",
    "X_train_image, X_val_image, X_test_image, \\\n",
    "y_train_image, y_val_image, y_test_image= prep16.split_data(image_data_s, y_s, random_state=42)\n",
    "\n",
    "y_train = to_categorical(y_train_polar, num_classes=3)\n",
    "y_val =to_categorical(y_val_polar, num_classes=3)\n",
    "y_test =to_categorical(y_test_polar, num_classes=3)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09d489c",
   "metadata": {},
   "source": [
    "# D1- Late Fusion* 08V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca49810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 26s 382ms/step - loss: 2.1501 - text_softmax_loss: 0.9829 - image_softmax_loss: 1.1672 - text_softmax_accuracy: 0.5435 - image_softmax_accuracy: 0.4816 - val_loss: 1.9655 - val_text_softmax_loss: 0.9498 - val_image_softmax_loss: 1.0158 - val_text_softmax_accuracy: 0.5483 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 23s 383ms/step - loss: 1.7258 - text_softmax_loss: 0.7127 - image_softmax_loss: 1.0131 - text_softmax_accuracy: 0.6860 - image_softmax_accuracy: 0.5284 - val_loss: 1.9816 - val_text_softmax_loss: 0.9423 - val_image_softmax_loss: 1.0392 - val_text_softmax_accuracy: 0.5811 - val_image_softmax_accuracy: 0.5483\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 24s 399ms/step - loss: 1.3530 - text_softmax_loss: 0.3959 - image_softmax_loss: 0.9572 - text_softmax_accuracy: 0.8490 - image_softmax_accuracy: 0.5612 - val_loss: 2.2085 - val_text_softmax_loss: 1.1460 - val_image_softmax_loss: 1.0625 - val_text_softmax_accuracy: 0.5647 - val_image_softmax_accuracy: 0.4374\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 25s 416ms/step - loss: 1.1316 - text_softmax_loss: 0.2019 - image_softmax_loss: 0.9297 - text_softmax_accuracy: 0.9284 - image_softmax_accuracy: 0.5617 - val_loss: 2.5468 - val_text_softmax_loss: 1.4480 - val_image_softmax_loss: 1.0987 - val_text_softmax_accuracy: 0.5298 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 28s 460ms/step - loss: 1.0253 - text_softmax_loss: 0.1379 - image_softmax_loss: 0.8874 - text_softmax_accuracy: 0.9538 - image_softmax_accuracy: 0.5941 - val_loss: 2.9801 - val_text_softmax_loss: 1.8190 - val_image_softmax_loss: 1.1612 - val_text_softmax_accuracy: 0.5092 - val_image_softmax_accuracy: 0.3634\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 29s 476ms/step - loss: 0.9626 - text_softmax_loss: 0.1066 - image_softmax_loss: 0.8559 - text_softmax_accuracy: 0.9612 - image_softmax_accuracy: 0.6116 - val_loss: 3.0662 - val_text_softmax_loss: 1.9163 - val_image_softmax_loss: 1.1499 - val_text_softmax_accuracy: 0.5175 - val_image_softmax_accuracy: 0.5072\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 32s 529ms/step - loss: 0.8943 - text_softmax_loss: 0.0954 - image_softmax_loss: 0.7989 - text_softmax_accuracy: 0.9653 - image_softmax_accuracy: 0.6431 - val_loss: 3.0219 - val_text_softmax_loss: 1.8688 - val_image_softmax_loss: 1.1531 - val_text_softmax_accuracy: 0.5277 - val_image_softmax_accuracy: 0.5257\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 30s 498ms/step - loss: 0.8524 - text_softmax_loss: 0.0894 - image_softmax_loss: 0.7630 - text_softmax_accuracy: 0.9656 - image_softmax_accuracy: 0.6703 - val_loss: 3.0639 - val_text_softmax_loss: 1.9561 - val_image_softmax_loss: 1.1078 - val_text_softmax_accuracy: 0.5298 - val_image_softmax_accuracy: 0.4086\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 34s 554ms/step - loss: 0.8105 - text_softmax_loss: 0.0858 - image_softmax_loss: 0.7246 - text_softmax_accuracy: 0.9671 - image_softmax_accuracy: 0.6873 - val_loss: 3.3796 - val_text_softmax_loss: 2.2818 - val_image_softmax_loss: 1.0978 - val_text_softmax_accuracy: 0.5031 - val_image_softmax_accuracy: 0.4846\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 32s 529ms/step - loss: 0.7545 - text_softmax_loss: 0.0801 - image_softmax_loss: 0.6744 - text_softmax_accuracy: 0.9666 - image_softmax_accuracy: 0.7114 - val_loss: 3.2063 - val_text_softmax_loss: 2.1143 - val_image_softmax_loss: 1.0920 - val_text_softmax_accuracy: 0.5092 - val_image_softmax_accuracy: 0.5195\n",
      "16/16 [==============================] - 2s 69ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2824    0.3117    0.2963        77\n",
      "           1     0.6838    0.7158    0.6995       278\n",
      "           2     0.4775    0.4015    0.4362       132\n",
      "\n",
      "    accuracy                         0.5667       487\n",
      "   macro avg     0.4812    0.4763    0.4773       487\n",
      "weighted avg     0.5644    0.5667    0.5644       487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# ----- Text branch -----\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=100)(text_input)\n",
    "t = LSTM(64, activation='tanh')(t)\n",
    "text_branch = Dense(64, activation='tanh')(t)\n",
    "text_out = Dense(3, activation='softmax', name='text_softmax')(text_branch)\n",
    "\n",
    "# ----- Image branch -----\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "i = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = Conv2D(128, (3, 3), activation='relu', padding='same')(i)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = BatchNormalization()(i)\n",
    "i = Conv2D(32, (3, 3), activation='relu', padding='same')(i)\n",
    "i = Dropout(0.5)(i)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = BatchNormalization()(i)\n",
    "i = Flatten()(i)\n",
    "image_branch = Dense(64, activation='relu')(i)\n",
    "image_out = Dense(3, activation='softmax', name='image_softmax')(image_branch)\n",
    "\n",
    "# ----- Define Output model-----\n",
    "model = Model(inputs=[text_input, image_input], outputs=[text_out, image_out])\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ---- Training\n",
    "history = model.fit(\n",
    "    [X_train_text, X_train_image], \n",
    "    [y_train, y_train],       # 如果兩分支label一樣就傳一份，否則可分開\n",
    "    validation_data=([X_val_text, X_val_image], [y_val, y_val]),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# ----- Prediction & Late Fusion -----\n",
    "# seperate Probs\n",
    "text_probs, image_probs = model.predict([X_test_text, X_test_image])\n",
    "# Late Fusion：AVG\n",
    "ensemble_probs = (text_probs + image_probs) / 2.0\n",
    "predictions = np.argmax(ensemble_probs, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "print(classification_report(true_labels, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff959de0",
   "metadata": {},
   "source": [
    "# D2 Early Fusion\n",
    "(13/05/2025 test ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66828de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 32s 496ms/step - loss: 1.1153 - accuracy: 0.5181 - val_loss: 1.0673 - val_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 37s 614ms/step - loss: 0.9887 - accuracy: 0.5471 - val_loss: 0.9764 - val_accuracy: 0.5462\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 39s 639ms/step - loss: 0.7832 - accuracy: 0.6372 - val_loss: 0.9057 - val_accuracy: 0.5626\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 36s 592ms/step - loss: 0.5391 - accuracy: 0.7569 - val_loss: 1.0539 - val_accuracy: 0.5421\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 41s 669ms/step - loss: 0.4271 - accuracy: 0.7928 - val_loss: 1.0978 - val_accuracy: 0.5544\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 40s 658ms/step - loss: 0.3516 - accuracy: 0.8475 - val_loss: 1.5173 - val_accuracy: 0.5154\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 36s 584ms/step - loss: 0.2729 - accuracy: 0.8937 - val_loss: 1.7871 - val_accuracy: 0.4682\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 33s 546ms/step - loss: 0.1848 - accuracy: 0.9315 - val_loss: 2.1130 - val_accuracy: 0.5216\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 34s 560ms/step - loss: 0.1261 - accuracy: 0.9574 - val_loss: 2.2673 - val_accuracy: 0.4949\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 34s 555ms/step - loss: 0.1002 - accuracy: 0.9682 - val_loss: 2.3482 - val_accuracy: 0.5154\n",
      "16/16 [==============================] - 2s 92ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2250    0.3506    0.2741        77\n",
      "           1     0.7388    0.6511    0.6922       278\n",
      "           2     0.4426    0.4091    0.4252       132\n",
      "\n",
      "    accuracy                         0.5380       487\n",
      "   macro avg     0.4688    0.4703    0.4638       487\n",
      "weighted avg     0.5773    0.5380    0.5537       487\n",
      "\n",
      "函數輸出: 499999500000\n",
      "EARLY FUSION TIME TAKEN：364.5164 秒\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Text branch\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=100)(text_input)\n",
    "t = LSTM(64, activation='tanh')(t)  # (batch, 64)\n",
    "\n",
    "# Image branch\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "i = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = Conv2D(128, (3, 3), activation='relu', padding='same')(i)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = BatchNormalization()(i)\n",
    "\n",
    "# Layer2:\n",
    "#i = Conv2D(32, (3, 3), activation=image_af, padding='same')(i)\n",
    "#i = Dropout(0.5)(i)  # Add a dropout layer\n",
    "#i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "#i = BatchNormalization()(i)\n",
    "\n",
    "i = Flatten()(i)  # (batch, N)\n",
    "\n",
    "\n",
    "# Early fusion: \n",
    "fusion = Concatenate()([t, i])  # (batch, 64+N)\n",
    "\n",
    "# Further processing\n",
    "fusion = Dense(128, activation='relu')(fusion)\n",
    "fusion = Dropout(0.5)(fusion)\n",
    "fusion = Dense(64, activation='relu')(fusion)\n",
    "output = Dense(3, activation='softmax')(fusion)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X_train_text, X_train_image], y_train, \n",
    "                    validation_data=([X_val_text, X_val_image], y_val),\n",
    "                    epochs=10, \n",
    "                    batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "#test_loss, test_acc = model.evaluate([X_test_text, X_test_image], y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "predictions_prob = model.predict([X_test_text, X_test_image])\n",
    "\n",
    "\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "output = some_function()\n",
    "print(f\"函數輸出: {output}\")\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"EARLY FUSION TIME TAKEN：{elapsed_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a9fe7",
   "metadata": {},
   "source": [
    "# *****----------- NN -----------*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca83118",
   "metadata": {},
   "source": [
    "# MultiHeadAttention Transformer + EfficientNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1aad4",
   "metadata": {},
   "source": [
    "# D3-Late Fusion* V08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0bdab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 116s 2s/step - loss: 2.0085 - text_softmax_loss: 1.0086 - image_softmax_loss: 0.9998 - text_softmax_accuracy: 0.5345 - image_softmax_accuracy: 0.5435 - val_loss: 2.0097 - val_text_softmax_loss: 1.0066 - val_image_softmax_loss: 1.0031 - val_text_softmax_accuracy: 0.5462 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 120s 2s/step - loss: 1.9994 - text_softmax_loss: 0.9965 - image_softmax_loss: 1.0029 - text_softmax_accuracy: 0.5466 - image_softmax_accuracy: 0.5451 - val_loss: 2.0028 - val_text_softmax_loss: 1.0065 - val_image_softmax_loss: 0.9963 - val_text_softmax_accuracy: 0.5462 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 130s 2s/step - loss: 1.8873 - text_softmax_loss: 0.8962 - image_softmax_loss: 0.9911 - text_softmax_accuracy: 0.5931 - image_softmax_accuracy: 0.5492 - val_loss: 1.8891 - val_text_softmax_loss: 0.8858 - val_image_softmax_loss: 1.0033 - val_text_softmax_accuracy: 0.5975 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 147s 2s/step - loss: 1.6285 - text_softmax_loss: 0.6278 - image_softmax_loss: 1.0007 - text_softmax_accuracy: 0.7363 - image_softmax_accuracy: 0.5461 - val_loss: 1.9876 - val_text_softmax_loss: 0.9840 - val_image_softmax_loss: 1.0036 - val_text_softmax_accuracy: 0.5832 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 133s 2s/step - loss: 1.3661 - text_softmax_loss: 0.3735 - image_softmax_loss: 0.9926 - text_softmax_accuracy: 0.8585 - image_softmax_accuracy: 0.5492 - val_loss: 2.2730 - val_text_softmax_loss: 1.2767 - val_image_softmax_loss: 0.9963 - val_text_softmax_accuracy: 0.5585 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 133s 2s/step - loss: 1.2034 - text_softmax_loss: 0.2121 - image_softmax_loss: 0.9913 - text_softmax_accuracy: 0.9214 - image_softmax_accuracy: 0.5492 - val_loss: 2.6506 - val_text_softmax_loss: 1.6486 - val_image_softmax_loss: 1.0020 - val_text_softmax_accuracy: 0.5626 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 135s 2s/step - loss: 1.1365 - text_softmax_loss: 0.1429 - image_softmax_loss: 0.9935 - text_softmax_accuracy: 0.9471 - image_softmax_accuracy: 0.5492 - val_loss: 2.6660 - val_text_softmax_loss: 1.6632 - val_image_softmax_loss: 1.0028 - val_text_softmax_accuracy: 0.5544 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 135s 2s/step - loss: 1.1067 - text_softmax_loss: 0.1123 - image_softmax_loss: 0.9943 - text_softmax_accuracy: 0.9566 - image_softmax_accuracy: 0.5492 - val_loss: 3.2524 - val_text_softmax_loss: 2.2512 - val_image_softmax_loss: 1.0012 - val_text_softmax_accuracy: 0.5503 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 135s 2s/step - loss: 1.0858 - text_softmax_loss: 0.0959 - image_softmax_loss: 0.9899 - text_softmax_accuracy: 0.9620 - image_softmax_accuracy: 0.5492 - val_loss: 2.9377 - val_text_softmax_loss: 1.9395 - val_image_softmax_loss: 0.9981 - val_text_softmax_accuracy: 0.5421 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 133s 2s/step - loss: 1.0796 - text_softmax_loss: 0.0896 - image_softmax_loss: 0.9900 - text_softmax_accuracy: 0.9623 - image_softmax_accuracy: 0.5492 - val_loss: 3.2649 - val_text_softmax_loss: 2.2690 - val_image_softmax_loss: 0.9959 - val_text_softmax_accuracy: 0.5277 - val_image_softmax_accuracy: 0.5462\n",
      "16/16 [==============================] - 9s 435ms/step\n",
      "\n",
      "Classification Report (Late Fusion):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2967    0.3506    0.3214        77\n",
      "           1     0.7175    0.6942    0.7057       278\n",
      "           2     0.4567    0.4394    0.4479       132\n",
      "\n",
      "    accuracy                         0.5708       487\n",
      "   macro avg     0.4903    0.4948    0.4917       487\n",
      "weighted avg     0.5803    0.5708    0.5750       487\n",
      "\n",
      "EARLY FUSION TIME TAKEN：1328.5270 秒\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.layers import (Input, Embedding, MultiHeadAttention, LayerNormalization,\n",
    "                                     GlobalAveragePooling1D, Dense, GlobalAveragePooling2D, Concatenate)\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "start_time = time.time()\n",
    "#------------------------------------------------------------\n",
    "# Text Transformer Encoder\n",
    "#------------------------------------------------------------\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=128)(text_input)\n",
    "attn = MultiHeadAttention(num_heads=4, key_dim=32)(t, t)\n",
    "attn = LayerNormalization()(attn)\n",
    "attn = GlobalAveragePooling1D()(attn)\n",
    "text_feat = Dense(64, activation='relu')(attn)\n",
    "\n",
    "\n",
    "text_prob = Dense(3, activation='softmax', name='text_softmax')(text_feat)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Image Encoder using EfficientNetB0\n",
    "#------------------------------------------------------------\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "base_model.trainable = False\n",
    "i = base_model.output\n",
    "i = GlobalAveragePooling2D()(i)\n",
    "image_feat = Dense(64, activation='relu')(i)\n",
    "\n",
    "\n",
    "image_prob = Dense(3, activation='softmax', name='image_softmax')(image_feat)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Model (Dual outputs)\n",
    "#------------------------------------------------------------\n",
    "model = Model(inputs=[text_input, image_input], outputs=[text_prob, image_prob])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={'text_softmax': 'categorical_crossentropy', 'image_softmax': 'categorical_crossentropy'},\n",
    "    loss_weights={'text_softmax': 1.0, 'image_softmax': 1.0},\n",
    "    metrics={'text_softmax': ['accuracy'], 'image_softmax': ['accuracy']}\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Training\n",
    "#------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    [X_train_text, X_train_image],\n",
    "    [y_train, y_train],\n",
    "    validation_data=([X_val_text, X_val_image], [y_val, y_val]),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluation — Late Fusion at inference\n",
    "#------------------------------------------------------------\n",
    "text_probs, image_probs = model.predict([X_test_text, X_test_image])\n",
    "\n",
    "# Plan A：Average\n",
    "ensemble_probs = 0.5 * (text_probs + image_probs)\n",
    "\n",
    "# Plan B：\n",
    "# import numpy as np\n",
    "# logp = np.log(text_probs + 1e-12) + np.log(image_probs + 1e-12)\n",
    "# ensemble_probs = np.exp(logp)\n",
    "# ensemble_probs /= ensemble_probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "predictions = np.argmax(ensemble_probs, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report (Late Fusion):\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "print(f\"EARLY FUSION TIME TAKEN：{elapsed_time:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5df03e",
   "metadata": {},
   "source": [
    "### 》》 D3 Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89cb9fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 110s 2s/step - loss: 2.0042 - text_softmax_loss: 1.0025 - image_softmax_loss: 1.0017 - text_softmax_accuracy: 0.5409 - image_softmax_accuracy: 0.5448 - val_loss: 1.9962 - val_text_softmax_loss: 0.9986 - val_image_softmax_loss: 0.9975 - val_text_softmax_accuracy: 0.5462 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 117s 2s/step - loss: 1.9729 - text_softmax_loss: 0.9792 - image_softmax_loss: 0.9937 - text_softmax_accuracy: 0.5492 - image_softmax_accuracy: 0.5492 - val_loss: 1.9832 - val_text_softmax_loss: 0.9812 - val_image_softmax_loss: 1.0020 - val_text_softmax_accuracy: 0.5483 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 112s 2s/step - loss: 1.7907 - text_softmax_loss: 0.7971 - image_softmax_loss: 0.9936 - text_softmax_accuracy: 0.6580 - image_softmax_accuracy: 0.5492 - val_loss: 1.9197 - val_text_softmax_loss: 0.9169 - val_image_softmax_loss: 1.0028 - val_text_softmax_accuracy: 0.5667 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 109s 2s/step - loss: 1.5627 - text_softmax_loss: 0.5685 - image_softmax_loss: 0.9942 - text_softmax_accuracy: 0.7638 - image_softmax_accuracy: 0.5492 - val_loss: 2.1063 - val_text_softmax_loss: 1.1089 - val_image_softmax_loss: 0.9974 - val_text_softmax_accuracy: 0.5606 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 108s 2s/step - loss: 1.3059 - text_softmax_loss: 0.3159 - image_softmax_loss: 0.9901 - text_softmax_accuracy: 0.8706 - image_softmax_accuracy: 0.5492 - val_loss: 2.3697 - val_text_softmax_loss: 1.3736 - val_image_softmax_loss: 0.9960 - val_text_softmax_accuracy: 0.5524 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 109s 2s/step - loss: 1.1794 - text_softmax_loss: 0.1885 - image_softmax_loss: 0.9909 - text_softmax_accuracy: 0.9258 - image_softmax_accuracy: 0.5492 - val_loss: 2.5444 - val_text_softmax_loss: 1.5467 - val_image_softmax_loss: 0.9977 - val_text_softmax_accuracy: 0.5175 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 108s 2s/step - loss: 1.1242 - text_softmax_loss: 0.1310 - image_softmax_loss: 0.9933 - text_softmax_accuracy: 0.9499 - image_softmax_accuracy: 0.5492 - val_loss: 2.8632 - val_text_softmax_loss: 1.8643 - val_image_softmax_loss: 0.9988 - val_text_softmax_accuracy: 0.5277 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 116s 2s/step - loss: 1.0902 - text_softmax_loss: 0.1005 - image_softmax_loss: 0.9897 - text_softmax_accuracy: 0.9615 - image_softmax_accuracy: 0.5492 - val_loss: 3.1485 - val_text_softmax_loss: 2.1520 - val_image_softmax_loss: 0.9965 - val_text_softmax_accuracy: 0.5503 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 110s 2s/step - loss: 1.0779 - text_softmax_loss: 0.0898 - image_softmax_loss: 0.9881 - text_softmax_accuracy: 0.9641 - image_softmax_accuracy: 0.5492 - val_loss: 3.1129 - val_text_softmax_loss: 2.1100 - val_image_softmax_loss: 1.0029 - val_text_softmax_accuracy: 0.5236 - val_image_softmax_accuracy: 0.5462\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 108s 2s/step - loss: 1.0764 - text_softmax_loss: 0.0853 - image_softmax_loss: 0.9911 - text_softmax_accuracy: 0.9648 - image_softmax_accuracy: 0.5492 - val_loss: 3.3700 - val_text_softmax_loss: 2.3727 - val_image_softmax_loss: 0.9973 - val_text_softmax_accuracy: 0.5359 - val_image_softmax_accuracy: 0.5462\n",
      "8/8 [==============================] - 7s 620ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3846    0.2597    0.3101        77\n",
      "           1     0.7297    0.6799    0.7039       278\n",
      "           2     0.4375    0.5833    0.5000       132\n",
      "\n",
      "    accuracy                         0.5873       487\n",
      "   macro avg     0.5173    0.5076    0.5047       487\n",
      "weighted avg     0.5960    0.5873    0.5864       487\n",
      "\n",
      "EARLY FUSION TIME TAKEN：1115.7302 秒\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.layers import (Input, Embedding, MultiHeadAttention, LayerNormalization,\n",
    "                                     GlobalAveragePooling1D, Dense, GlobalAveragePooling2D, Concatenate)\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "start_time = time.time()\n",
    "#------------------------------------------------------------\n",
    "# Text branch -> own classifier\n",
    "#------------------------------------------------------------\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=128)(text_input)\n",
    "t = MultiHeadAttention(num_heads=4, key_dim=32)(t, t)\n",
    "t = LayerNormalization()(t)\n",
    "t = GlobalAveragePooling1D()(t)\n",
    "t_feat = Dense(64, activation='relu', name='text_feat')(t)\n",
    "text_logits = Dense(3, activation=None, name='text_logits')(t_feat)     # logits\n",
    "text_probs  = Dense(3, activation='softmax', name='text_softmax')(t_feat)  # probs \n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Image branch -> own classifier\n",
    "#------------------------------------------------------------\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "base_model.trainable = False\n",
    "i = base_model.output\n",
    "i = GlobalAveragePooling2D()(i)\n",
    "i_feat = Dense(64, activation='relu', name='image_feat')(i)\n",
    "image_logits = Dense(3, activation=None, name='image_logits')(i_feat)\n",
    "image_probs  = Dense(3, activation='softmax', name='image_softmax')(i_feat)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Build multi-output model \n",
    "#------------------------------------------------------------\n",
    "model = Model(inputs=[text_input, image_input],\n",
    "              outputs=[text_probs, image_probs])  # or [text_logits, image_logits]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={'text_softmax':'categorical_crossentropy', 'image_softmax':'categorical_crossentropy'},\n",
    "    loss_weights={'text_softmax':1.0, 'image_softmax':1.0},\n",
    "    metrics={'text_softmax':['accuracy'], 'image_softmax':['accuracy']}\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Training \n",
    "#------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    [X_train_text, X_train_image],\n",
    "    {'text_softmax': y_train, 'image_softmax': y_train},\n",
    "    validation_data=([X_val_text, X_val_image], {'text_softmax': y_val, 'image_softmax': y_val}),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Prediction & LATE FUSION\n",
    "#------------------------------------------------------------\n",
    "text_probs, image_probs = model.predict([X_test_text, X_test_image], batch_size=64)\n",
    "\n",
    "# 1) Ave\n",
    "ensemble_probs_avg = 0.5 * text_probs + 0.5 * image_probs\n",
    "\n",
    "# 2) log：softmax(log p1 + log p2)\n",
    "logp_t = np.log(text_probs + 1e-12)\n",
    "logp_i = np.log(image_probs + 1e-12)\n",
    "ensemble_probs_geo = np.exp(logp_t + logp_i)\n",
    "ensemble_probs_geo = ensemble_probs_geo / ensemble_probs_geo.sum(axis=1, keepdims=True)\n",
    "\n",
    "# 3) With Weighting α（eg α∈[0,1]）\n",
    "alpha = 0.6  \n",
    "ensemble_probs_alpha = alpha * text_probs + (1 - alpha) * image_probs\n",
    "\n",
    "pred = np.argmax(ensemble_probs_geo, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "print(classification_report(true_labels, pred, digits=4))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"EARLY FUSION TIME TAKEN：{elapsed_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2beeddf",
   "metadata": {},
   "source": [
    "> # Late Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775639c",
   "metadata": {},
   "source": [
    "## Transformer+ EfficentNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038354e",
   "metadata": {},
   "source": [
    "> ## D4 Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbc6b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 101s 2s/step - loss: 0.9965 - accuracy: 0.5492 - val_loss: 0.9965 - val_accuracy: 0.5462\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 116s 2s/step - loss: 0.9904 - accuracy: 0.5492 - val_loss: 1.0003 - val_accuracy: 0.5462\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 113s 2s/step - loss: 0.9889 - accuracy: 0.5492 - val_loss: 0.9976 - val_accuracy: 0.5462\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 116s 2s/step - loss: 0.9372 - accuracy: 0.5546 - val_loss: 0.8993 - val_accuracy: 0.5749\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 118s 2s/step - loss: 0.7254 - accuracy: 0.6770 - val_loss: 0.9828 - val_accuracy: 0.5852\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 115s 2s/step - loss: 0.5560 - accuracy: 0.7484 - val_loss: 1.1301 - val_accuracy: 0.5708\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 120s 2s/step - loss: 0.4727 - accuracy: 0.7792 - val_loss: 1.2314 - val_accuracy: 0.5729\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 156s 3s/step - loss: 0.4140 - accuracy: 0.7990 - val_loss: 1.3193 - val_accuracy: 0.5339\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 111s 2s/step - loss: 0.3458 - accuracy: 0.8542 - val_loss: 1.6015 - val_accuracy: 0.5257\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 112s 2s/step - loss: 0.2611 - accuracy: 0.8978 - val_loss: 1.8391 - val_accuracy: 0.5441\n",
      "16/16 [==============================] - 7s 341ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2727    0.3117    0.2909        77\n",
      "           1     0.7101    0.7050    0.7076       278\n",
      "           2     0.4390    0.4091    0.4235       132\n",
      "\n",
      "    accuracy                         0.5626       487\n",
      "   macro avg     0.4740    0.4753    0.4740       487\n",
      "weighted avg     0.5675    0.5626    0.5647       487\n",
      "\n",
      "\n",
      "函數輸出: 499999500000\n",
      "程式碼執行時間：1185.4309 秒\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Adagrad, RMSprop\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, GlobalAveragePooling1D, LayerNormalization, Concatenate\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Timer Start\n",
    "#------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "def some_function():\n",
    "    result = 0\n",
    "    for i in range(1000000):\n",
    "        result += i\n",
    "    return result\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Text Transformer Encoder\n",
    "#------------------------------------------------------------\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=128)(text_input)\n",
    "t = MultiHeadAttention(num_heads=4, key_dim=32)(t, t)\n",
    "t = LayerNormalization()(t)\n",
    "t = GlobalAveragePooling1D()(t)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Image Encoder using EfficientNetB0\n",
    "#------------------------------------------------------------\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "base_model.trainable = False\n",
    "i = base_model.output\n",
    "i = GlobalAveragePooling2D()(i)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Early Fusion\n",
    "#------------------------------------------------------------\n",
    "concatenated = Concatenate()([t, i])\n",
    "fusion = Dense(64, activation='relu')(concatenated)\n",
    "fusion = Dense(64, activation='relu')(fusion)\n",
    "output = Dense(3, activation='softmax')(fusion)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Training\n",
    "#------------------------------------------------------------\n",
    "history = model.fit([X_train_text, X_train_image], y_train, \n",
    "                    validation_data=([X_val_text, X_val_image], y_val),\n",
    "                    epochs=10, \n",
    "                    batch_size=64)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluation\n",
    "#------------------------------------------------------------\n",
    "predictions_prob = model.predict([X_test_text, X_test_image])\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "#------------------------------------------------------------\n",
    "# Timer End\n",
    "#------------------------------------------------------------\n",
    "output = some_function()\n",
    "print(f\"\\n函數輸出: {output}\")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"程式碼執行時間：{elapsed_time:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a053d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
