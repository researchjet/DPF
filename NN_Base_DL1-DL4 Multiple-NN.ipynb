{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c16aabe",
   "metadata": {},
   "source": [
    "# D1-D4 Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a28f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (Activation, Attention, Bidirectional, Concatenate, Conv1D,\n",
    "                           Dense, Dropout, Embedding, Flatten, GlobalMaxPooling1D,\n",
    "                           Input, Layer, LSTM, MaxPooling1D, Multiply, Permute,\n",
    "                           RepeatVector, Reshape, SpatialDropout1D, TimeDistributed)\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sys\n",
    "sys.path.append('./cs16')\n",
    "import cs16.prep as prep16\n",
    "import cs16.plot as plot16\n",
    "import cs16.build as build16\n",
    "import cs16.DPF as DPF\n",
    "imagesize = 64\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data_type = input(\"Enter '1. single' for single dataset or '2. multi' for multiple dataset: \")\n",
    "data_type = '2'\n",
    "if data_type == '1':\n",
    "    file_path = 'single.txt'\n",
    "    folder_path = './data/MVSA/single/'\n",
    "elif data_type == '2':\n",
    "    file_path = 'multi.txt'\n",
    "    folder_path = './data/MVSA/multiple/'\n",
    "else:\n",
    "    print(\"Invalid input. Please enter either 'single' or 'multi'.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49123e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Download stopwords and punkt tokenizer if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def nlp_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs using regex\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem words using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the preprocess_text function to the 'tweet' column of the dataframe\n",
    "df['tweet'] = df['tweet'].apply(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5e4873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text, y_text = prep16.preprocess_text(df)\n",
    "X_train_text, X_val_text, X_test_text, \\\n",
    "y_train_text, y_val_text, y_test_text = prep16.split_data(X_text, y_text, random_state=42)\n",
    "\n",
    "X_polar, y_polar = prep16.preprocess_text(df,label = 'polarity')\n",
    "X_train_polar, X_val_polar, X_test_polar, \\\n",
    "y_train_polar, y_val_polar, y_test_polar = prep16.split_data(X_polar, y_polar, random_state=42)\n",
    "\n",
    "image_data_s, image_label_s = prep16.preprocess_images(df, folder_path, imagesize)\n",
    "y_s = to_categorical(image_label_s, num_classes=3)\n",
    "\n",
    "X_train_image, X_val_image, X_test_image, \\\n",
    "y_train_image, y_val_image, y_test_image= prep16.split_data(image_data_s, y_s, random_state=42)\n",
    "\n",
    "y_train = to_categorical(y_train_polar, num_classes=3)\n",
    "y_val =to_categorical(y_val_polar, num_classes=3)\n",
    "y_test =to_categorical(y_test_polar, num_classes=3)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df9e67",
   "metadata": {},
   "source": [
    "> # D1 - Late Fusion V08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07c6608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 96s 386ms/step - loss: 1.5219 - text_softmax_loss: 0.7123 - image_softmax_loss: 0.8096 - text_softmax_accuracy: 0.7474 - image_softmax_accuracy: 0.7294 - val_loss: 1.5270 - val_text_softmax_loss: 0.6263 - val_image_softmax_loss: 0.9007 - val_text_softmax_accuracy: 0.7786 - val_image_softmax_accuracy: 0.7227\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 115s 471ms/step - loss: 1.3239 - text_softmax_loss: 0.5848 - image_softmax_loss: 0.7391 - text_softmax_accuracy: 0.7727 - image_softmax_accuracy: 0.7470 - val_loss: 1.3499 - val_text_softmax_loss: 0.6470 - val_image_softmax_loss: 0.7028 - val_text_softmax_accuracy: 0.7766 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 128s 524ms/step - loss: 1.1957 - text_softmax_loss: 0.4646 - image_softmax_loss: 0.7311 - text_softmax_accuracy: 0.8162 - image_softmax_accuracy: 0.7458 - val_loss: 1.4388 - val_text_softmax_loss: 0.7331 - val_image_softmax_loss: 0.7057 - val_text_softmax_accuracy: 0.7488 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 1.0829 - text_softmax_loss: 0.3575 - image_softmax_loss: 0.7254 - text_softmax_accuracy: 0.8617 - image_softmax_accuracy: 0.7468 - val_loss: 1.5701 - val_text_softmax_loss: 0.8662 - val_image_softmax_loss: 0.7039 - val_text_softmax_accuracy: 0.7345 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 150s 614ms/step - loss: 0.9951 - text_softmax_loss: 0.2768 - image_softmax_loss: 0.7182 - text_softmax_accuracy: 0.8975 - image_softmax_accuracy: 0.7469 - val_loss: 1.7405 - val_text_softmax_loss: 0.9921 - val_image_softmax_loss: 0.7483 - val_text_softmax_accuracy: 0.7108 - val_image_softmax_accuracy: 0.7509\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 143s 588ms/step - loss: 0.9355 - text_softmax_loss: 0.2241 - image_softmax_loss: 0.7114 - text_softmax_accuracy: 0.9182 - image_softmax_accuracy: 0.7484 - val_loss: 1.8911 - val_text_softmax_loss: 1.1778 - val_image_softmax_loss: 0.7133 - val_text_softmax_accuracy: 0.6944 - val_image_softmax_accuracy: 0.7684\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 136s 556ms/step - loss: 0.8850 - text_softmax_loss: 0.1872 - image_softmax_loss: 0.6978 - text_softmax_accuracy: 0.9302 - image_softmax_accuracy: 0.7498 - val_loss: 1.9722 - val_text_softmax_loss: 1.2602 - val_image_softmax_loss: 0.7120 - val_text_softmax_accuracy: 0.6995 - val_image_softmax_accuracy: 0.7673\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 130s 531ms/step - loss: 0.8509 - text_softmax_loss: 0.1610 - image_softmax_loss: 0.6898 - text_softmax_accuracy: 0.9393 - image_softmax_accuracy: 0.7536 - val_loss: 2.1699 - val_text_softmax_loss: 1.4683 - val_image_softmax_loss: 0.7016 - val_text_softmax_accuracy: 0.6923 - val_image_softmax_accuracy: 0.7694\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 140s 572ms/step - loss: 0.8196 - text_softmax_loss: 0.1450 - image_softmax_loss: 0.6746 - text_softmax_accuracy: 0.9460 - image_softmax_accuracy: 0.7558 - val_loss: 2.2610 - val_text_softmax_loss: 1.5401 - val_image_softmax_loss: 0.7209 - val_text_softmax_accuracy: 0.6985 - val_image_softmax_accuracy: 0.7596\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 144s 592ms/step - loss: 0.7883 - text_softmax_loss: 0.1280 - image_softmax_loss: 0.6603 - text_softmax_accuracy: 0.9511 - image_softmax_accuracy: 0.7575 - val_loss: 2.4578 - val_text_softmax_loss: 1.7438 - val_image_softmax_loss: 0.7140 - val_text_softmax_accuracy: 0.6877 - val_image_softmax_accuracy: 0.7678\n",
      "61/61 [==============================] - 5s 77ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1695    0.0787    0.1075       254\n",
      "           1     0.7781    0.8953    0.8326      1461\n",
      "           2     0.3061    0.1948    0.2381       231\n",
      "\n",
      "    accuracy                         0.7055      1946\n",
      "   macro avg     0.4179    0.3896    0.3927      1946\n",
      "weighted avg     0.6426    0.7055    0.6674      1946\n",
      "\n",
      "EARLY FUSION TIME TAKEN：1329.9321 秒\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "start_time = time.time()\n",
    "# ----- Text branch -----\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=100)(text_input)\n",
    "t = LSTM(64, activation='tanh')(t)\n",
    "text_branch = Dense(64, activation='tanh')(t)\n",
    "text_out = Dense(3, activation='softmax', name='text_softmax')(text_branch)\n",
    "\n",
    "# ----- Image branch -----\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "i = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = Conv2D(128, (3, 3), activation='relu', padding='same')(i)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = BatchNormalization()(i)\n",
    "i = Conv2D(32, (3, 3), activation='relu', padding='same')(i)\n",
    "i = Dropout(0.5)(i)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = BatchNormalization()(i)\n",
    "i = Flatten()(i)\n",
    "image_branch = Dense(64, activation='relu')(i)\n",
    "image_out = Dense(3, activation='softmax', name='image_softmax')(image_branch)\n",
    "\n",
    "# ----- Define output model -----\n",
    "model = Model(inputs=[text_input, image_input], outputs=[text_out, image_out])\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ---- Trainig----\n",
    "history = model.fit(\n",
    "    [X_train_text, X_train_image], \n",
    "    [y_train, y_train],       \n",
    "    validation_data=([X_val_text, X_val_image], [y_val, y_val]),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# ----- Prediction & Late Fusion -----\n",
    "# Seperated Probs\n",
    "text_probs, image_probs = model.predict([X_test_text, X_test_image])\n",
    "# Late Fusion：AVG\n",
    "ensemble_probs = (text_probs + image_probs) / 2.0\n",
    "predictions = np.argmax(ensemble_probs, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Time taken\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# print\n",
    "print(f\"EARLY FUSION TIME TAKEN：{elapsed_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff959de0",
   "metadata": {},
   "source": [
    "> # D2 Multiple Early Fusion\n",
    "(13/05/2025 test ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d846fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 118s 475ms/step - loss: 0.8103 - accuracy: 0.7368 - val_loss: 0.6461 - val_accuracy: 0.7720\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 127s 519ms/step - loss: 0.6267 - accuracy: 0.7585 - val_loss: 0.6401 - val_accuracy: 0.7756\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 138s 565ms/step - loss: 0.5369 - accuracy: 0.7872 - val_loss: 0.6862 - val_accuracy: 0.7571\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 141s 577ms/step - loss: 0.4543 - accuracy: 0.8173 - val_loss: 0.7564 - val_accuracy: 0.7375\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 153s 626ms/step - loss: 0.3942 - accuracy: 0.8398 - val_loss: 0.8690 - val_accuracy: 0.7216\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 155s 635ms/step - loss: 0.3355 - accuracy: 0.8668 - val_loss: 1.0444 - val_accuracy: 0.6995\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 168s 690ms/step - loss: 0.2829 - accuracy: 0.8909 - val_loss: 1.1095 - val_accuracy: 0.7016\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 170s 697ms/step - loss: 0.2368 - accuracy: 0.9106 - val_loss: 1.3002 - val_accuracy: 0.7062\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 145s 595ms/step - loss: 0.2001 - accuracy: 0.9256 - val_loss: 1.5016 - val_accuracy: 0.6949\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 147s 601ms/step - loss: 0.1708 - accuracy: 0.9363 - val_loss: 1.6630 - val_accuracy: 0.7293\n",
      "61/61 [==============================] - 6s 83ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1429    0.0866    0.1078       254\n",
      "           1     0.7823    0.8857    0.8308      1461\n",
      "           2     0.3333    0.1991    0.2493       231\n",
      "\n",
      "    accuracy                         0.6999      1946\n",
      "   macro avg     0.4195    0.3905    0.3960      1946\n",
      "weighted avg     0.6456    0.6999    0.6674      1946\n",
      "\n",
      "函數輸出: 499999500000\n",
      "EARLY FUSION TIME TAKEN：1467.3117 秒\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# start\n",
    "start_time = time.time()\n",
    "\n",
    "# Text branch\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=100)(text_input)\n",
    "t = LSTM(64, activation='tanh')(t)  # (batch, 64)\n",
    "\n",
    "# Image branch\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "i = Conv2D(64, (3, 3), activation='relu', padding='same')(image_input)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = Conv2D(128, (3, 3), activation='relu', padding='same')(i)\n",
    "i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "i = BatchNormalization()(i)\n",
    "\n",
    "# Layer2:\n",
    "#i = Conv2D(32, (3, 3), activation=image_af, padding='same')(i)\n",
    "#i = Dropout(0.5)(i)  # Add a dropout layer\n",
    "#i = MaxPooling2D(pool_size=(2, 2))(i)\n",
    "#i = BatchNormalization()(i)\n",
    "\n",
    "i = Flatten()(i)  # (batch, N)\n",
    "\n",
    "\n",
    "# Early fusion: \n",
    "fusion = Concatenate()([t, i])  # (batch, 64+N)\n",
    "\n",
    "\n",
    "fusion = Dense(128, activation='relu')(fusion)\n",
    "fusion = Dropout(0.5)(fusion)\n",
    "fusion = Dense(64, activation='relu')(fusion)\n",
    "output = Dense(3, activation='softmax')(fusion)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit([X_train_text, X_train_image], y_train, \n",
    "                    validation_data=([X_val_text, X_val_image], y_val),\n",
    "                    epochs=10, \n",
    "                    batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "#test_loss, test_acc = model.evaluate([X_test_text, X_test_image], y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "predictions_prob = model.predict([X_test_text, X_test_image])\n",
    "\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "output = some_function()\n",
    "print(f\"Function: {output}\")\n",
    "# End \n",
    "end_time = time.time()\n",
    "\n",
    "#Compute time taken\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Out\n",
    "print(f\"EARLY FUSION TIME TAKEN：{elapsed_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d1c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e01357b",
   "metadata": {},
   "source": [
    "# MultiHeadAttention Transformer + EfficientNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e0495",
   "metadata": {},
   "source": [
    "# D3-Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a720f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 104s 406ms/step - loss: 1.4873 - text_softmax_loss: 0.7394 - image_softmax_loss: 0.7479 - text_softmax_accuracy: 0.7436 - image_softmax_accuracy: 0.7440 - val_loss: 1.3603 - val_text_softmax_loss: 0.6575 - val_image_softmax_loss: 0.7028 - val_text_softmax_accuracy: 0.7704 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 113s 462ms/step - loss: 1.4108 - text_softmax_loss: 0.6654 - image_softmax_loss: 0.7454 - text_softmax_accuracy: 0.7560 - image_softmax_accuracy: 0.7469 - val_loss: 1.3326 - val_text_softmax_loss: 0.6236 - val_image_softmax_loss: 0.7089 - val_text_softmax_accuracy: 0.7833 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 123s 504ms/step - loss: 1.3388 - text_softmax_loss: 0.5924 - image_softmax_loss: 0.7465 - text_softmax_accuracy: 0.7722 - image_softmax_accuracy: 0.7469 - val_loss: 1.3580 - val_text_softmax_loss: 0.6601 - val_image_softmax_loss: 0.6980 - val_text_softmax_accuracy: 0.7720 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 124s 510ms/step - loss: 1.2683 - text_softmax_loss: 0.5244 - image_softmax_loss: 0.7439 - text_softmax_accuracy: 0.7903 - image_softmax_accuracy: 0.7469 - val_loss: 1.4221 - val_text_softmax_loss: 0.7232 - val_image_softmax_loss: 0.6989 - val_text_softmax_accuracy: 0.7550 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 121s 495ms/step - loss: 1.1768 - text_softmax_loss: 0.4340 - image_softmax_loss: 0.7428 - text_softmax_accuracy: 0.8290 - image_softmax_accuracy: 0.7469 - val_loss: 1.5074 - val_text_softmax_loss: 0.8078 - val_image_softmax_loss: 0.6995 - val_text_softmax_accuracy: 0.7411 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 125s 511ms/step - loss: 1.1061 - text_softmax_loss: 0.3627 - image_softmax_loss: 0.7434 - text_softmax_accuracy: 0.8573 - image_softmax_accuracy: 0.7469 - val_loss: 1.6450 - val_text_softmax_loss: 0.9470 - val_image_softmax_loss: 0.6981 - val_text_softmax_accuracy: 0.7298 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 129s 527ms/step - loss: 1.0445 - text_softmax_loss: 0.3009 - image_softmax_loss: 0.7436 - text_softmax_accuracy: 0.8796 - image_softmax_accuracy: 0.7469 - val_loss: 1.7886 - val_text_softmax_loss: 1.0897 - val_image_softmax_loss: 0.6989 - val_text_softmax_accuracy: 0.7108 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 140s 576ms/step - loss: 0.9995 - text_softmax_loss: 0.2563 - image_softmax_loss: 0.7431 - text_softmax_accuracy: 0.8929 - image_softmax_accuracy: 0.7469 - val_loss: 2.1539 - val_text_softmax_loss: 1.4547 - val_image_softmax_loss: 0.6993 - val_text_softmax_accuracy: 0.7011 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 131s 535ms/step - loss: 0.9673 - text_softmax_loss: 0.2256 - image_softmax_loss: 0.7417 - text_softmax_accuracy: 0.9075 - image_softmax_accuracy: 0.7469 - val_loss: 2.2165 - val_text_softmax_loss: 1.5169 - val_image_softmax_loss: 0.6995 - val_text_softmax_accuracy: 0.7124 - val_image_softmax_accuracy: 0.7704\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 119s 487ms/step - loss: 0.9421 - text_softmax_loss: 0.1997 - image_softmax_loss: 0.7424 - text_softmax_accuracy: 0.9150 - image_softmax_accuracy: 0.7469 - val_loss: 2.5094 - val_text_softmax_loss: 1.8086 - val_image_softmax_loss: 0.7008 - val_text_softmax_accuracy: 0.6893 - val_image_softmax_accuracy: 0.7704\n",
      "61/61 [==============================] - 11s 156ms/step\n",
      "\n",
      "Classification Report (Late Fusion):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1622    0.0945    0.1194       254\n",
      "           1     0.7804    0.8734    0.8243      1461\n",
      "           2     0.2883    0.2035    0.2386       231\n",
      "\n",
      "    accuracy                         0.6922      1946\n",
      "   macro avg     0.4103    0.3904    0.3941      1946\n",
      "weighted avg     0.6413    0.6922    0.6628      1946\n",
      "\n",
      "EARLY FUSION TIME TAKEN：1240.9110 秒\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.layers import (Input, Embedding, MultiHeadAttention, LayerNormalization,\n",
    "                                     GlobalAveragePooling1D, Dense, GlobalAveragePooling2D, Concatenate)\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "start_time = time.time()\n",
    "#------------------------------------------------------------\n",
    "# Text Transformer Encoder\n",
    "#------------------------------------------------------------\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=128)(text_input)\n",
    "attn = MultiHeadAttention(num_heads=4, key_dim=32)(t, t)\n",
    "attn = LayerNormalization()(attn)\n",
    "attn = GlobalAveragePooling1D()(attn)\n",
    "text_feat = Dense(64, activation='relu')(attn)\n",
    "\n",
    "\n",
    "text_prob = Dense(3, activation='softmax', name='text_softmax')(text_feat)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Image Encoder using EfficientNetB0\n",
    "#------------------------------------------------------------\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "base_model.trainable = False\n",
    "i = base_model.output\n",
    "i = GlobalAveragePooling2D()(i)\n",
    "image_feat = Dense(64, activation='relu')(i)\n",
    "\n",
    "\n",
    "image_prob = Dense(3, activation='softmax', name='image_softmax')(image_feat)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Model \n",
    "#------------------------------------------------------------\n",
    "model = Model(inputs=[text_input, image_input], outputs=[text_prob, image_prob])\n",
    "\n",
    "# loss/metric\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss={'text_softmax': 'categorical_crossentropy', 'image_softmax': 'categorical_crossentropy'},\n",
    "    loss_weights={'text_softmax': 1.0, 'image_softmax': 1.0},\n",
    "    metrics={'text_softmax': ['accuracy'], 'image_softmax': ['accuracy']}\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Training（兩個輸出都用同一份標籤；如需可分開）\n",
    "#------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    [X_train_text, X_train_image],\n",
    "    [y_train, y_train],\n",
    "    validation_data=([X_val_text, X_val_image], [y_val, y_val]),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluation — Late Fusion at inference\n",
    "#------------------------------------------------------------\n",
    "text_probs, image_probs = model.predict([X_test_text, X_test_image])\n",
    "\n",
    "# Plan A：AVERAGE\n",
    "ensemble_probs = 0.5 * (text_probs + image_probs)\n",
    "\n",
    "# Plan B：\n",
    "# import numpy as np\n",
    "# logp = np.log(text_probs + 1e-12) + np.log(image_probs + 1e-12)\n",
    "# ensemble_probs = np.exp(logp)\n",
    "# ensemble_probs /= ensemble_probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "predictions = np.argmax(ensemble_probs, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report (Late Fusion):\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "print(f\"EARLY FUSION TIME TAKEN：{elapsed_time:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c55a5",
   "metadata": {},
   "source": [
    "> ##  Early Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adcb947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 380s 2s/step - loss: 0.7479 - accuracy: 0.7469 - val_loss: 0.6965 - val_accuracy: 0.7704\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 366s 1s/step - loss: 0.6916 - accuracy: 0.7494 - val_loss: 0.6494 - val_accuracy: 0.7730\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 358s 1s/step - loss: 0.6179 - accuracy: 0.7635 - val_loss: 0.6290 - val_accuracy: 0.7802\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 367s 2s/step - loss: 0.5546 - accuracy: 0.7801 - val_loss: 0.6697 - val_accuracy: 0.7786\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 377s 2s/step - loss: 0.5073 - accuracy: 0.7957 - val_loss: 0.6990 - val_accuracy: 0.7571\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 365s 1s/step - loss: 0.4665 - accuracy: 0.8124 - val_loss: 0.7509 - val_accuracy: 0.7386\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 353s 1s/step - loss: 0.4387 - accuracy: 0.8264 - val_loss: 0.8365 - val_accuracy: 0.7036\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 375s 2s/step - loss: 0.4095 - accuracy: 0.8388 - val_loss: 0.9263 - val_accuracy: 0.7149\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 355s 1s/step - loss: 0.3645 - accuracy: 0.8607 - val_loss: 1.0006 - val_accuracy: 0.7201\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 427s 2s/step - loss: 0.3154 - accuracy: 0.8809 - val_loss: 1.1148 - val_accuracy: 0.7114\n",
      "61/61 [==============================] - 25s 377ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1450    0.0748    0.0987       254\n",
      "           1     0.7924    0.8542    0.8221      1461\n",
      "           2     0.3167    0.3290    0.3227       231\n",
      "\n",
      "    accuracy                         0.6901      1946\n",
      "   macro avg     0.4180    0.4193    0.4145      1946\n",
      "weighted avg     0.6514    0.6901    0.6684      1946\n",
      "\n",
      "\n",
      "函數輸出: 499999500000\n",
      "程式碼執行時間：3750.2781 秒\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam, Adagrad, RMSprop\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, GlobalAveragePooling1D, LayerNormalization, Concatenate\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Timer Start\n",
    "#------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "def some_function():\n",
    "    result = 0\n",
    "    for i in range(1000000):\n",
    "        result += i\n",
    "    return result\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Text Transformer Encoder\n",
    "#------------------------------------------------------------\n",
    "text_input = Input(shape=(200,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=128)(text_input)\n",
    "t = MultiHeadAttention(num_heads=4, key_dim=32)(t, t)\n",
    "t = LayerNormalization()(t)\n",
    "t = GlobalAveragePooling1D()(t)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Image Encoder using EfficientNetB0\n",
    "#------------------------------------------------------------\n",
    "image_input = Input(shape=(64, 64, 3), name='image_input')\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=image_input)\n",
    "base_model.trainable = False\n",
    "i = base_model.output\n",
    "i = GlobalAveragePooling2D()(i)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Early Fusion\n",
    "#------------------------------------------------------------\n",
    "concatenated = Concatenate()([t, i])\n",
    "fusion = Dense(64, activation='relu')(concatenated)\n",
    "fusion = Dense(64, activation='relu')(fusion)\n",
    "output = Dense(3, activation='softmax')(fusion)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Training\n",
    "#------------------------------------------------------------\n",
    "history = model.fit([X_train_text, X_train_image], y_train, \n",
    "                    validation_data=([X_val_text, X_val_image], y_val),\n",
    "                    epochs=10, \n",
    "                    batch_size=64)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Evaluation\n",
    "#------------------------------------------------------------\n",
    "predictions_prob = model.predict([X_test_text, X_test_image])\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "#------------------------------------------------------------\n",
    "# Timer End\n",
    "#------------------------------------------------------------\n",
    "output = some_function()\n",
    "print(f\"\\nOutput: {output}\")\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"程式碼執行時間：{elapsed_time:.4f} 秒\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a748176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
