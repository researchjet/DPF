{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c24a5eb",
   "metadata": {},
   "source": [
    "# VLM/MLLM -D5/D6 Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a28f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter '1. single' for single dataset or '2. multi' for multiple dataset: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (Activation, Attention, Bidirectional, Concatenate, Conv1D,\n",
    "                           Dense, Dropout, Embedding, Flatten, GlobalMaxPooling1D,\n",
    "                           Input, Layer, LSTM, MaxPooling1D, Multiply, Permute,\n",
    "                           RepeatVector, Reshape, SpatialDropout1D, TimeDistributed)\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import sys\n",
    "sys.path.append('./cs16')\n",
    "import cs16.prep as prep16\n",
    "import cs16.plot as plot16\n",
    "import cs16.build as build16\n",
    "imagesize = 64\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#data_type = input(\"Enter '1. single' for single dataset or '2. multi' for multiple dataset: \")\n",
    "data_type = '1'\n",
    "if data_type == '1':\n",
    "    file_path = 'single.txt'\n",
    "    folder_path = './data/MVSA/single/'\n",
    "elif data_type == '2':\n",
    "    file_path = 'multi.txt'\n",
    "    folder_path = './data/MVSA/multiple/'\n",
    "else:\n",
    "    print(\"Invalid input. Please enter either 'single' or 'multi'.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49123e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Download stopwords and punkt tokenizer if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def nlp_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs using regex\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem words using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the preprocess_text function to the 'tweet' column of the dataframe\n",
    "df['tweet'] = df['tweet'].apply(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5e4873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'single.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgsize=96\n",
    "X_text, y_text = prep16.preprocess_text(df)\n",
    "X_train_text, X_val_text, X_test_text, \\\n",
    "y_train_text, y_val_text, y_test_text = prep16.split_data(X_text, y_text, random_state=42)\n",
    "\n",
    "X_polar, y_polar = prep16.preprocess_text(df,label = 'polarity')\n",
    "X_train_polar, X_val_polar, X_test_polar, \\\n",
    "y_train_polar, y_val_polar, y_test_polar = prep16.split_data(X_polar, y_polar, random_state=42)\n",
    "\n",
    "image_data_s, image_label_s = prep16.preprocess_images(df, folder_path, imagesize =imgsize)\n",
    "y_s = to_categorical(image_label_s, num_classes=3)\n",
    "\n",
    "X_train_image, X_val_image, X_test_image, \\\n",
    "y_train_image, y_val_image, y_test_image= prep16.split_data(image_data_s, y_s, random_state=42)\n",
    "\n",
    "y_train = to_categorical(y_train_polar, num_classes=3)\n",
    "y_val =to_categorical(y_val_polar, num_classes=3)\n",
    "y_test =to_categorical(y_test_polar, num_classes=3)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "29c1dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4869, 96, 96, 3), TensorShape([3895, 96, 96, 3]), (4869, 3))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data_s.shape, X_train_image.shape, y_s.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730e7d3",
   "metadata": {},
   "source": [
    "# 1. EARLY FUSION TinyCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc37054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "All PyTorch model weights were used when initializing TFCLIPModel.\n",
      "\n",
      "All the weights of TFCLIPModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 20s 281ms/step - loss: 3.8199 - accuracy: 0.3664 - val_loss: 3.5140 - val_accuracy: 0.4559\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 15s 241ms/step - loss: 3.5921 - accuracy: 0.4496 - val_loss: 3.4675 - val_accuracy: 0.4764\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 15s 252ms/step - loss: 3.5301 - accuracy: 0.4680 - val_loss: 3.4283 - val_accuracy: 0.4805\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 15s 255ms/step - loss: 3.4896 - accuracy: 0.4770 - val_loss: 3.3948 - val_accuracy: 0.4743\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 17s 276ms/step - loss: 3.4641 - accuracy: 0.4760 - val_loss: 3.3653 - val_accuracy: 0.4825\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 16s 261ms/step - loss: 3.4206 - accuracy: 0.4816 - val_loss: 3.3382 - val_accuracy: 0.4990\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 16s 257ms/step - loss: 3.3670 - accuracy: 0.5030 - val_loss: 3.3107 - val_accuracy: 0.4949\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 16s 261ms/step - loss: 3.3425 - accuracy: 0.5004 - val_loss: 3.2861 - val_accuracy: 0.5031\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 17s 271ms/step - loss: 3.2943 - accuracy: 0.5109 - val_loss: 3.2641 - val_accuracy: 0.5051\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 16s 258ms/step - loss: 3.2874 - accuracy: 0.5142 - val_loss: 3.2430 - val_accuracy: 0.5092\n",
      "16/16 [==============================] - 3s 120ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2941    0.0649    0.1064        77\n",
      "           1     0.5868    0.8022    0.6778       278\n",
      "           2     0.3556    0.2424    0.2883       132\n",
      "\n",
      "    accuracy                         0.5339       487\n",
      "   macro avg     0.4122    0.3698    0.3575       487\n",
      "weighted avg     0.4779    0.5339    0.4819       487\n",
      "\n",
      "TinyCLIP+MobileNetV2- EARLY Fusion：477.0369 秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from transformers import AutoTokenizer, TFCLIPModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer Start\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TinyCLIP/CLIP TEXT\n",
    "# ------------------------------------------------------------\n",
    "text_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = TFCLIPModel.from_pretrained(text_model_name)\n",
    "text_encoder.trainable = False\n",
    "\n",
    "def get_text_features(texts, processor, model, maxlen=77):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=maxlen,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    features = model.get_text_features(\n",
    "        input_ids=encodings[\"input_ids\"]\n",
    "    )\n",
    "    return features.numpy()\n",
    "\n",
    "# Text \n",
    "X_train_text = [str(t) for t in X_train_text]\n",
    "X_val_text = [str(t) for t in X_val_text]\n",
    "X_test_text = [str(t) for t in X_test_text]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MobileNetV2 IMAGE \n",
    "# ------------------------------------------------------------\n",
    "image_input = Input(shape=(imgsize, imgsize, 3), name='image_input')\n",
    "mobilenet = tf.keras.applications.MobileNetV2(include_top=False, \n",
    "                                            weights='imagenet', \n",
    "                                            input_shape=(imgsize, imgsize, 3))\n",
    "mobilenet.trainable = False\n",
    "image_features = mobilenet(image_input)\n",
    "\n",
    "#text\n",
    "text_input = Input(shape=(512,), name='text_input')  # CLIP 512 maxlength\n",
    "text_embedding = Dense(128, activation='relu')(text_input)   \n",
    "text_embedding = Reshape((1, 1, 128))(text_embedding)  #Preparing for fusion\n",
    "\n",
    "# image\n",
    "# Expand text features -> to the size of image features\n",
    "h, w = image_features.shape[1], image_features.shape[2]\n",
    "text_features_expanded = tf.keras.layers.UpSampling2D(size=(h, w))(text_embedding)\n",
    "\n",
    "# Early Fusion: Feature Level \n",
    "fusion = Concatenate(axis=-1)([image_features, text_features_expanded])\n",
    "\n",
    "# reduce dim\n",
    "x = GlobalAveragePooling2D()(fusion)\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "#Reduce Overfitting\n",
    "opti = tf.keras.optimizers.AdamW(learning_rate=1e-5, clipnorm=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# to vector for fusion\n",
    "X_train_text_feat = get_text_features(X_train_text, tokenizer, text_encoder)\n",
    "X_val_text_feat = get_text_features(X_val_text, tokenizer, text_encoder)\n",
    "X_test_text_feat = get_text_features(X_test_text, tokenizer, text_encoder)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training\n",
    "# ------------------------------------------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_text_feat, X_train_image], y_train,\n",
    "    validation_data=([X_val_text_feat, X_val_image], y_val),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Report\n",
    "# ------------------------------------------------------------\n",
    "predictions_prob = model.predict([X_test_text_feat, X_test_image])\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer End\n",
    "end_time = time.time()\n",
    "print(f\"TinyCLIP+MobileNetV2- EARLY Fusion：{end_time - start_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a88ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3f7dfa",
   "metadata": {},
   "source": [
    "# 2. LATE FUSION TinyCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66078113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "All PyTorch model weights were used when initializing TFCLIPModel.\n",
      "\n",
      "All the weights of TFCLIPModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "61/61 [==============================] - 26s 359ms/step - loss: 2.1419 - accuracy: 0.4750 - val_loss: 2.0038 - val_accuracy: 0.5503\n",
      "Epoch 2/10\n",
      "61/61 [==============================] - 20s 324ms/step - loss: 1.9444 - accuracy: 0.5358 - val_loss: 1.8621 - val_accuracy: 0.5503\n",
      "Epoch 3/10\n",
      "61/61 [==============================] - 19s 315ms/step - loss: 1.7969 - accuracy: 0.5497 - val_loss: 1.7420 - val_accuracy: 0.5503\n",
      "Epoch 4/10\n",
      "61/61 [==============================] - 19s 319ms/step - loss: 1.6764 - accuracy: 0.5535 - val_loss: 1.6416 - val_accuracy: 0.5503\n",
      "Epoch 5/10\n",
      "61/61 [==============================] - 20s 321ms/step - loss: 1.5808 - accuracy: 0.5653 - val_loss: 1.5562 - val_accuracy: 0.5503\n",
      "Epoch 6/10\n",
      "61/61 [==============================] - 20s 334ms/step - loss: 1.4878 - accuracy: 0.5756 - val_loss: 1.4857 - val_accuracy: 0.5544\n",
      "Epoch 7/10\n",
      "61/61 [==============================] - 23s 377ms/step - loss: 1.4163 - accuracy: 0.5815 - val_loss: 1.4225 - val_accuracy: 0.5606\n",
      "Epoch 8/10\n",
      "61/61 [==============================] - 22s 365ms/step - loss: 1.3467 - accuracy: 0.5879 - val_loss: 1.3672 - val_accuracy: 0.5647\n",
      "Epoch 9/10\n",
      "61/61 [==============================] - 23s 378ms/step - loss: 1.2931 - accuracy: 0.5926 - val_loss: 1.3247 - val_accuracy: 0.5606\n",
      "Epoch 10/10\n",
      "61/61 [==============================] - 21s 344ms/step - loss: 1.2330 - accuracy: 0.6026 - val_loss: 1.2892 - val_accuracy: 0.5667\n",
      "16/16 [==============================] - 4s 177ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.0390    0.0732        77\n",
      "           1     0.6070    0.8777    0.7176       278\n",
      "           2     0.5000    0.3030    0.3774       132\n",
      "\n",
      "    accuracy                         0.5893       487\n",
      "   macro avg     0.5690    0.4066    0.3894       487\n",
      "weighted avg     0.5769    0.5893    0.5235       487\n",
      "\n",
      "TinyCLIP+MobileNetV2-LATE Fusion：519.4859 秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from transformers import AutoTokenizer, TFCLIPModel\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer Start\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TinyCLIP/CLIP\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "#text_model_name = \"openai/clip-vit-base-patch32\"  # 若有 TinyCLIP 的 TF 版本可替換\n",
    "text_model_name = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(text_model_name)\n",
    "text_encoder = CLIPModel.from_pretrained(text_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = TFCLIPModel.from_pretrained(text_model_name)\n",
    "# TFCLIPModel ->input_ids, Not necessarily needs attention_mask\n",
    "text_encoder.trainable = False\n",
    "def get_text_features(texts, processor, model, maxlen=77):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=maxlen,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    # TinyCLIP/CLIP TF -> get_text_features\n",
    "    features = model.get_text_features(\n",
    "        input_ids=encodings[\"input_ids\"]\n",
    "    )\n",
    "    return features.numpy()\n",
    "\n",
    "# Transfer to [str]\n",
    "X_train_text = [str(t) for t in X_train_text]\n",
    "X_val_text = [str(t) for t in X_val_text]\n",
    "X_test_text = [str(t) for t in X_test_text]\n",
    "\n",
    "#Extract FIRST : text data into numerical features\n",
    "X_train_text_feat = get_text_features(X_train_text, tokenizer, text_encoder)\n",
    "X_val_text_feat = get_text_features(X_val_text, tokenizer, text_encoder)\n",
    "X_test_text_feat = get_text_features(X_test_text, tokenizer, text_encoder)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MobileNetV2 \n",
    "# ------------------------------------------------------------\n",
    "mobilenet = tf.keras.applications.MobileNetV2(include_top=False, \n",
    "                                              weights='imagenet', \n",
    "                                              input_shape=(imgsize, imgsize, 3))\n",
    "mobilenet.trainable = False\n",
    "image_input = Input(shape=(imgsize, imgsize, 3), name='image_input')\n",
    "\n",
    "image_features = mobilenet(image_input)\n",
    "image_features = GlobalAveragePooling2D()(image_features)\n",
    "image_features = Dense(64, activation='relu')(image_features)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fusion\n",
    "# ------------------------------------------------------------\n",
    "text_input = Input(shape=(X_train_text_feat.shape[1],), name='text_feat_input')\n",
    "fusion = Concatenate()([text_input, image_features])\n",
    "fusion = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(fusion)\n",
    "fusion = Dropout(0.5)(fusion)  # \n",
    "output = Dense(3, activation='softmax')(fusion)\n",
    "\n",
    "model = Model(inputs=[text_input, image_input], outputs=output)\n",
    "opti = tf.keras.optimizers.AdamW(learning_rate=1e-4, clipnorm=1.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Training\n",
    "# ------------------------------------------------------------\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_text_feat, X_train_image], y_train,\n",
    "    validation_data=([X_val_text_feat, X_val_image], y_val),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Report\n",
    "# ------------------------------------------------------------\n",
    "predictions_prob = model.predict([X_test_text_feat, X_test_image])\n",
    "predictions = np.argmax(predictions_prob, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Timer End\n",
    "end_time = time.time()\n",
    "print(f\"TinyCLIP+MobileNetV2-LATE Fusion：{end_time - start_time:.4f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1a67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
